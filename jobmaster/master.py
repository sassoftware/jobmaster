#/usr/bin/python
#
# Copyright (c) 2005-2008 rPath, Inc.
#
# All rights reserved
#

import os, sys
import inspect
import logging
import math
import simplejson
import tempfile
import threading
import time
import traceback
import shutil
import signal
import socket
import stat
import weakref

from jobmaster import master_error
from jobmaster import imagecache
from jobmaster import templateserver
from jobmaster import xencfg, xenmac
from jobmaster.resource import AutoMountResource, LVMResource
from jobmaster.util import getRunningKernel
from jobmaster.util import rewriteFile, logCall, getIP

from mcp import mcp_log
from mcp import queue
from mcp import response
from mcp import client
from mcp import slavestatus

from conary.lib import cfgtypes, util
from conary import conarycfg
from conary import conaryclient
from conary.conaryclient import cmdline
from conary.deps import deps
from conary import versions

log = logging.getLogger('jobmaster.master')


CONFIG_PATH = os.path.join(os.path.sep, 'srv', 'rbuilder', 'jobmaster',
                           'config.d', 'runtime')

LVM_PATH = os.path.join(os.path.sep, 'dev', 'mapper')

def getAvailableArchs(arch):
    if arch in ('i686', 'i586', 'i486', 'i386'):
        return ('x86',)
    elif arch == 'x86_64':
        return ('x86', 'x86_64')

def controlMethod(func):
    func._controlMethod = True
    return func


PROTOCOL_VERSIONS = set([1])

filterArgs = lambda d, *args: dict([x for x in d.iteritems() \
                                        if x[0] not in args])

getJsversion = lambda troveSpec: str(versions.VersionFromString( \
        cmdline.parseTroveSpec(troveSpec)[1]).trailingRevision())

def protocols(protocolList):
    if type(protocolList) in (int, long):
        protocolList = (protocolList,)
    def deco(func):
        def wrapper(self, *args, **kwargs):
            if kwargs.get('protocolVersion') in protocolList:
                return func(self, *args,
                            **(filterArgs(kwargs, 'protocolVersion')))
            else:
                raise master_error.ProtocolError(\
                    'Unsupported ProtocolVersion: %s' % \
                        str(kwargs.get('protocolVersion')))
        return wrapper
    return deco

def catchErrors(func):
    def wrapper(self, *args, **kwargs):
        try:
            func(self, *args, **kwargs)
        except:
            exc, e, bt = sys.exc_info()
            try:
                log.error(''.join(traceback.format_tb(bt)))
                log.error(e)
            except:
                print >> sys.stderr, "couldn't log error", e
    return wrapper

class MasterConfig(client.MCPClientConfig):
    basePath = os.path.join(os.path.sep, 'srv', 'rbuilder', 'jobmaster')

    logFile = os.path.join(os.path.sep, 'var', 'log', 'rbuilder', 'jobmaster.log')
    logLevel = (mcp_log.CfgLogLevel, 'INFO')

    slaveLimit = (cfgtypes.CfgInt, 1)
    # maxSlaveLimit == 0 means unlimited
    maxSlaveLimit = (cfgtypes.CfgInt, 0)
    nodeName = (cfgtypes.CfgString, None)
    templateCache = os.path.join(basePath, 'anaconda-templates')

    # Jobslave parameters
    debugMode = (cfgtypes.CfgBool, False)
    lvmVolumeName = 'vg00'
    minSlaveSize = (cfgtypes.CfgInt, 1024) # scratch space in MB
    slaveMemory = (cfgtypes.CfgInt, 512) # memory in MB
    slaveSubnet = 'fdf0:dbe6:3760::/48'

    # This should either be the URI of a rBuilder, or "self" to use the
    # local IP. It must be an rBuilder since the template generation code
    # assumes it can find a conaryrc file here.
    conaryProxy = 'self'

def copyHosts(targetFile, masterIP):
    '''
    Intelligently copy the master's hosts file to the jobslave.

     * localhost entry will be generated from scratch
     * entries other than "localhost" from the
       jobmaster that point at loopback will be
       copied using the jobmaster's IP instead
     * Other entries are copied verbatim
    '''

    inHostsFile = open('/etc/hosts')
    outHostsFile = open(targetFile, 'w')

    def writeLine(ip, aliases):
        print >>outHostsFile, '%-16s %s' % (ip, ' '.join(aliases))

    print >>outHostsFile, '# Automatically generated by jobmaster'
    writeLine('127.0.0.1', ['localhost.localdomain', 'localhost'])
    writeLine('::1', ['localhost6.localdomain6', 'localhost6'])

    masterAliases = set()

    hostname = socket.getfqdn()
    if 'localhost' not in hostname:
        masterAliases.add(hostname)

    for line in inHostsFile.readlines():
        if '#' in line:
            line = line[:line.index('#')]
        parts = line.split()
        if len(parts) < 2:
            continue
        ip, aliases = parts[0], set(parts[1:])

        # Skip loopback, but keep the extra aliases to put towards
        # the jobmaster's real IP
        if ip == '127.0.0.1' or ip == '::1':
            masterAliases.update(aliases)
            continue

        # Other entries are copied verbatim
        writeLine(ip, aliases)

    # Now generate an entry for the jobmaster
    masterAliases -= set(['localhost', 'localhost.localdomain', 'localhost6',
        'localhost6.localdomain6'])
    writeLine(masterIP, masterAliases)


def writeJobSlaveConfig(cfg, mntPoint):
    # write python SlaveConfig
    cfgPath = os.path.join(mntPoint, 'srv', 'jobslave', 'config.d',
                          'runtime')
    util.mkdirChain(os.path.split(cfgPath)[0])
    self.writeSlaveConfig(cfgPath, cfg)

    # insert jobData into slave
    dataPath = os.path.join(mntPoint, 'srv', 'jobslave', 'data')
    f = open(dataPath, 'w')
    f.write(simplejson.dumps(self.data))
    f.close()

    # write init script settings
    initSettings = os.path.join(mntPoint, 'etc', 'sysconfig',
                                'slave_runtime')
    util.mkdirChain(os.path.split(initSettings)[0])
    f = open(initSettings, 'w')

    # the host IP address is domU IP address + 127 of the last quad
    quads = [int(x) for x in self.ip.split(".")]
    masterIP = ".".join(str(x) for x in quads[:3] + [(quads[3]+127) % 256])

    f.write('MASTER_IP=%s' % masterIP)
    f.close()
    entitlementsDir = os.path.join(os.path.sep, 'srv',
                                   'rbuilder', 'entitlements')
    if os.path.exists(entitlementsDir):
        util.copytree(entitlementsDir,
                      os.path.join(mntPoint, 'srv', 'jobslave'))

    # set up networking inside domU
    ifcfg = os.path.join(mntPoint, 'etc', 'sysconfig', 'network-scripts', 'ifcfg-eth0')
    rewriteFile(ifcfg + ".template", ifcfg, dict(masterip = masterIP, ipaddr = self.ip))

    resolv = os.path.join(mntPoint, 'etc', 'resolv.conf')
    util.copyfile('/etc/resolv.conf', resolv)

    # Intelligently copy /etc/hosts into the jobslave.
    copyHosts(os.path.join(mntPoint, 'etc', 'hosts'), masterIP)


class JobMaster(object):
    def __init__(self, cfg):
        mcp_log.addRootLogger(level=cfg.logLevel,
            format ='%(asctime)s %(levelname)s %(message)s',
            filename = cfg.logFile,
            filemode='a')

        if cfg.nodeName is None:
            cfg.nodeName = getIP() or '127.0.0.1'
        if cfg.conaryProxy == 'self':
            cfg.conaryProxy = 'http://%s/' % getIP()
        elif not cfg.conaryProxy.endswith('/'):
            cfg.conaryProxy += '/'

        self.cfg = cfg
        self.jobQueue = queue.MultiplexedQueue(cfg.queueHost, cfg.queuePort,
                                       namespace = cfg.namespace,
                                       timeOut = 0, queueLimit = cfg.slaveLimit)
        self.arch = os.uname()[-1]
        archs = getAvailableArchs(self.arch)
        assert archs, "Unknown machine architecture."
        for arch in archs:
            self.jobQueue.addDest('job:' + arch)
        self.controlTopic = queue.Topic(cfg.queueHost, cfg.queuePort,
                                       'control', namespace = cfg.namespace,
                                       timeOut = 0)
        self.response = response.MCPResponse(self.cfg.nodeName, cfg)
        self.slaves = {}
        self.handlers = {}

        signal.signal(signal.SIGTERM, self.catchSignal)
        signal.signal(signal.SIGINT, self.catchSignal)

        self.templateServer, self.templateServerReaper = \
                templateserver.getServer(
                    self.cfg.templateCache, hostname=self.cfg.nodeName,
                    tmpDir=os.path.join(self.cfg.basePath, 'tmp'),
                    conaryProxy=cfg.conaryProxy)

        self.conaryCfg = conarycfg.ConaryConfiguration(True)
        self.conaryCfg.initializeFlavors()
        self.conaryCli = conaryclient.ConaryClient(cfg)

        # Determine the currently running kernel once on startup. The only
        # way it could ever change is with kexec, which we definitely don't
        # support.
        self.kernelData = getRunningKernel()

        log.info('started jobmaster: %s' % self.cfg.nodeName)
        self.lastHeartbeat = 0

    @catchErrors
    def checkControlTopic(self):
        dataStr = self.controlTopic.read()
        while dataStr:
            data = simplejson.loads(dataStr)
            node = data.get('node', '')
            if node in ('masters', self.cfg.nodeName):
                action = data['action']
                kwargs = dict([(str(x[0]), x[1]) for x in data.iteritems() \
                                   if x[0] not in ('node', 'action')])
                memberDict = dict([ \
                        x for x in inspect.getmembers( \
                            self, lambda x: callable(x))])
                if action in memberDict:
                    func = memberDict[action]
                    if '_controlMethod' in func.__dict__:
                        return func(**kwargs)
                    else:
                        raise master_error.ProtocolError( \
                            "Action '%s' is not a control method" % action)
                else:
                    raise master_error.ProtocolError( \
                        "Control method '%s' does not exist" % action)
            elif node.split(':')[0] == self.cfg.nodeName:
                #check list of slaves and ensure it's really up
                slaveName = node.split(':')[1]
                p = os.popen("xm list| awk '{print $1;}'")
                if slaveName not in p.read():
                    log.info("Detected missing slave.")
                    self.sendStatus()
                p.close()
            dataStr = self.controlTopic.read()

    def getMaxSlaves(self):
        p = os.popen('xm info | grep total_memory | sed "s/.* //"')
        mem = p.read().strip()
        if mem.isdigit():
            mem = int(mem)
        else:
            log.error("can't determine host memory. assuming zero.")
            mem = 0
        p.close()

        # get the dom0 used mem.
        # note, we need to assume this value is constant so it doesn't play
        # well with the balloon driver.
        p = os.popen("xm list 0 | tail -n 1 | awk '{print $3};'")
        dom0Mem = p.read().strip()
        if dom0Mem.isdigit():
            dom0Mem = int(dom0Mem)
        else:
            log.error("can't determine host dom0 Memory. assuming zero.")
            dom0Mem = 0
        p.close()

        # reserve memory for non-slave usage
        mem -= dom0Mem
        count = max(0, mem / self.cfg.slaveMemory)
        if not count:
            log.error("not enough memory: jobmaster cannot support slaves at all")
        if self.cfg.maxSlaveLimit:
            count = min(max(0, self.cfg.maxSlaveLimit), count)
        return count

    def realSlaveLimit(self):
        p = os.popen('xm info | grep total_memory | sed "s/.* //"')
        mem = p.read().strip()
        if mem.isdigit():
            mem = int(mem)
        else:
            log.error("can't determine host memory. assuming zero.")
            mem = 0
        p.close()

        # add up the total memory of all domains
        # tradeoff. more complex bash script for far less complex test cases
        p = os.popen("n = 0; for x in `xm list | grep -v 'Mem(MiB)' | awk '{print $3}'`; do n=$(( $n + $x )); done; echo $n")
        domMem = p.read().strip()
        if domMem.isdigit():
            domMem = int(domMem)
        else:
            log.error("can't determine domain memory. assuming zero.")
            domMem = 0
        p.close()

        mem -= domMem
        # we need to put some swag into this number for the sake of xen.
        # we simply cannot eat *all* the RAM on the box
        mem -= 64
        count = max(0, mem / self.cfg.slaveMemory)
        if not count:
            log.error("memory squeeze won't allow for more slaves")
        if self.cfg.maxSlaveLimit:
            count = min(max(0, self.cfg.maxSlaveLimit), count)
        return count

    def resolveTroveSpec(self, troveSpec):
        # this function is designed to ensure a partial NVF can be resolved to
        # a full NVF for caching and creation purposes.
        n, v, f = cmdline.parseTroveSpec(troveSpec)
        troves = self.conaryClient.getRepos().findTrove(None, (n, v, None))
        refXen = deps.parseFlavor('xen, domU')
        troves = [x for x in troves if x[2].stronglySatisfies(refXen) \
                      and x[2].stronglySatisfies(f)]
        if not troves:
            log.warning("Found no troves when looking for slaves. "
                        "This is almost certainly unwanted behavior. "
                        "Falling back to: %s" % troveSpec)
            return troveSpec
        if len(troves) > 1:
            # compare each pair of troves. take the difference between them
            # and see if the result satisfies f. this roughly translates to
            # a concept of "narrowest match" because an exact arch will be
            # preferred over a multi-arch trove.
            refTrove = troves[0]
            for trv in troves:
                if trv != refTrove:
                    if (trv[2].difference(refTrove[2])).satisfies(f):
                        refTrove = trv
            troves = [refTrove]
        res = '%s=%s[%s]' % troves[0]
        log.info("Using %s to satisfy %s for slave" % (res, troveSpec))
        return res

    def handleSlaveStart(self, data):
        troveSpec = self.resolveTroveSpec(data['jobSlaveNVF'])
        handler = SlaveHandler(self, troveSpec, data)
        self.handlers[handler.start()] = handler

    def handleSlaveStop(self, slaveId):
        slaveName = slaveId.split(':')[-1]
        handler = None
        if slaveName in self.slaves:
            handler = self.slaves[slaveName]
            del self.slaves[slaveName]
        elif slaveName in self.handlers:
            handler = self.handlers[slaveName]
            del self.handlers[slaveName]
        if handler:
            handler.stop()
            self.checkSlaveCount()
        self.sendStatus()

    def checkSlaveCount(self):
        currentSlaves = len(self.slaves) + len(self.handlers)
        totalCount = self.jobQueue.queueLimit + currentSlaves
        if totalCount != self.cfg.slaveLimit:
            slaveLimit = max(0, min(self.cfg.slaveLimit - currentSlaves,
                    self.realSlaveLimit() - len(self.handlers)))
            if slaveLimit != self.jobQueue.queueLimit:
                log.info('Setting limit of job queue to: %s' % str(slaveLimit))
            self.jobQueue.setLimit(slaveLimit)

    @catchErrors
    def checkJobQueue(self):
        dataStr = self.jobQueue.read()
        if dataStr:
            data = simplejson.loads(dataStr)
            if data['protocolVersion'] == 1:
                self.handleSlaveStart(data)
            else:
                log.error('Invalid Protocol Version %d' % \
                              data['protocolVersion'])
                # FIXME: protocol problem
                # should implement some sort of error feedback to MCP

    @catchErrors
    def checkHandlers(self):
        """Move slaves from 'being started' to 'active'

        Handlers are used to start slaves. Once a handler is done starting a
        slave, we know it's active."""
        for slaveName in [x[0] for x in self.handlers.iteritems() \
                              if not x[1].isAlive()]:
            self.slaves[slaveName] = self.handlers[slaveName]
            del self.handlers[slaveName]

    @catchErrors
    def checkSlaves(self):
        currentSlaves = [x for x in self.slaves]
        if currentSlaves:
            p = os.popen("xm list | awk '{print $1;}' | grep -v 'Name' | grep -v 'Domain-0'")
            runningSlaves = [x.strip() for x in p.readlines()]
            for slave in [x for x in currentSlaves if x not in runningSlaves]:
                log.error('slave: %s unexpectedly died' % slave)
                self.handleSlaveStop(slave)

    @catchErrors
    def heartbeat(self):
        curTime = time.time()
        if (curTime - self.lastHeartbeat) > 30:
            self.lastHeartbeat = curTime
            self.checkSlaveCount()
            self.sendStatus()

    def run(self):
        self.running = True
        self.templateServer.start()
        self.templateServerReaper.start()
        try:
            while self.running:
                self.checkJobQueue()
                self.checkControlTopic()
                self.checkHandlers()
                self.checkSlaves()
                self.heartbeat()
                time.sleep(0.1)
        finally:
            self.stopAllSlaves()
            self.response.masterOffline()
            self.disconnect()
            self.templateServer.stop()
            self.templateServerReaper.stop()

    def flushJobs(self):
        self.jobQueue.setLimit(0)
        dataStr = self.jobQueue.read()
        count = 0
        while dataStr:
            data = simplejson.loads(dataStr)
            if data.get('protocolVersion') == 1:
                jobId = data['UUID']
                slaveName = 'deadSlave%d' % count
                count += 1
                self.slaveStatus(slaveName, slavestatus.BUILDING, '', jobId)
                self.slaveStatus(slaveName, slavestatus.OFFLINE, '', jobId)
            else:
                # need to tell the mcp a job was lost
                log.error('Invalid Protocol Version %d' % \
                              data['protocolVersion'])
            dataStr = self.jobQueue.read()

    def stopAllSlaves(self):
        self.flushJobs()
        for slaveId in self.slaves.keys() + self.handlers.keys():
            self.handleSlaveStop(slaveId)

    def catchSignal(self, sig, frame):
        log.info('caught signal: %d' % sig)
        self.running = False

    def disconnect(self):
        log.info('stopping jobmaster')
        self.running = False
        self.jobQueue.disconnect()
        self.controlTopic.disconnect()
        del self.response

    def sendStatus(self):
        log.debug('sending master status')
        slaves = self.slaves.keys()
        handlers = [x[0] for x in self.handlers.iteritems() if x[1].isOnline()]
        self.response.masterStatus( \
            arch = self.arch, limit = self.cfg.slaveLimit,
            slaveIds = ['%s:%s' % (self.cfg.nodeName, x) for x in \
                            (slaves + handlers)])

    def slaveStatus(self, slaveName, status, slaveType, jobId):
        log.info('sending slave status: %s %s %s' % \
                     (self.cfg.nodeName + ':' + slaveName, status, slaveType))
        self.response.slaveStatus(self.cfg.nodeName + ':' + slaveName,
                                  status, slaveType, jobId)

    def getBestProtocol(self, protocols):
        common = PROTOCOL_VERSIONS.intersection(protocols)
        return common and max(common) or 0

    @controlMethod
    def checkVersion(self, protocols):
        log.info('asked for protocol compatibility: %s' % str(protocols))
        self.response.protocol(self.getBestProtocol(protocols))

    @controlMethod
    @protocols((1,))
    def slaveLimit(self, limit):
        log.info('asked to set slave limit to %d' % limit)
        # ensure we don't exceed environmental constraints

        newLimit = min(limit, self.getMaxSlaves())
        if limit != newLimit:
            log.warning('System cannot support %d. setting slave limit to %d' \
                    % (limit, newLimit))
            limit = newLimit
        limit = max(limit, 0)
        self.cfg.slaveLimit = limit


        f = open(CONFIG_PATH, 'w')
        f.write('slaveLimit %d\n' % limit)
        f.close()
        self.sendStatus()

        self.checkSlaveCount()

    @controlMethod
    @protocols((1,))
    def clearImageCache(self):
        "DEPRECATED"
        pass

    @controlMethod
    @protocols((1,))
    def status(self):
        log.info('status requested')
        self.sendStatus()

    @controlMethod
    @protocols((1,))
    def stopSlave(self, slaveId):
        log.info('stopping slave: %s' % slaveId)
        self.handleSlaveStop(slaveId)

def main(cfg):
    jobMaster = JobMaster(cfg)
    jobMaster.run()

def runDaemon():
    cfg = MasterConfig()
    cfg.read(os.path.join(os.path.sep, 'srv', 'rbuilder', 'jobmaster',
                          'config'))

    pidFile = os.path.join(os.path.sep, 'var', 'run', 'jobmaster.pid')
    if os.path.exists(pidFile):
        f = open(pidFile)
        pid = f.read()
        f.close()
        statPath = os.path.join(os.path.sep, 'proc', pid, 'stat')
        if os.path.exists(statPath):
            f = open(statPath)
            name = f.read().split()[1][1:-1]
            if name == 'jobmaster':
                print >> sys.stderr, "Job Master already running as: %s" % pid
                sys.stderr.flush()
                sys.exit(-1)
            else:
                # pidfile doesn't point to a job master
                os.unlink(pidFile)
        else:
            # pidfile is stale
            os.unlink(pidFile)
    pid = os.fork()
    if not pid:
        devNull = os.open(os.devnull, os.O_RDWR)
        os.dup2(devNull, sys.stdout.fileno())
        os.dup2(devNull, sys.stderr.fileno())
        os.dup2(devNull, sys.stdin.fileno())
        os.close(devNull)
        pid = os.fork()
        if not pid:
            os.setpgid(0, 0)
            f = open(pidFile, 'w')
            f.write(str(os.getpid()))
            f.close()
            main(cfg)
            os.unlink(pidFile)
